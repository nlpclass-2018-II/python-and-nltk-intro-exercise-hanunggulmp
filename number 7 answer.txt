#Point 1
Stemming usually refers to a crude heuristic process that chops off the ends of words 
in the hope of achieving this goal correctly most of the time, and often includes the 
removal of derivational affixes.

Lemmatization usually refers to doing things properly with the use of a vocabulary 
and morphological analysis of words, normally aiming to remove inflectional endings 
only and to return the base or dictionary form of a word, which is known as the lemma. 

#Point 2
Do you the tokenization function could run successfully on any language? yes